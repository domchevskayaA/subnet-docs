<h2>The Problem</h2>

<p>
  Currently, the cost barrier to training state-of-the-art language models is
  extremely high. GPT-4 is suspected to have cost more than
  <a
    href="https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/"
    >$100 million dollars</a
  >
  to train, while Anthropic's CEO predicts that training SOTA models will cost
  <a href="https://x.com/sarahdingwang/status/1694415161837453534"
    >$1 billion dollars this year followed by $10 billion dollars next year</a
  >. This means that only a small oligarchy of well-funded tech giants now have
  the ability to train these models.
</p>

<p>
  As these models grow more intelligent and have more impact on our daily lives,
  so do their owners. They end up deciding how they should be censored and whose
  values they should incorporate. In effect, this means we get to be governed by
  AI trained on a
  <a
    href="https://www.notion.so/Decentralised-Distributed-Training-fd21bdfa72294dfeab8fb092770212b9?pvs=21"
    >constitution</a
  >
  we never voted for.
</p>

<p>
  Blockchain, the Decentralised movement, and more specifically Bittensor have
  proved that they can provide alternatives to this centralised approach by
  incentivising the masses to pool their resources together to carry out useful
  work. As Const often mentions, the collective amount of compute that goes into
  mining Bitcoin far exceeds the compute of any Google, Microsoft, OpenAI or
  Anthropic data centres.
</p>

<p>
  Granted, Machine Learning requires a different type of compute, but if a
  decentralised mechanism is able to incentivise that specific type of compute
  in a similar way whilst accurately validating it, then in theory it can have
  access to a similar size of compute, if not larger, to train an extremely
  large single model.
</p>

<h2>The Solution</h2>

<p>
  Our proposed solution is a subnetwork that incentivises Compute, Bandwidth and
  Latency. The compute helps power the training of a miner's local version of a
  model and the bandwidth and latency helps power the averaging of each miner's
  local model weights using an operation called
  <a href="https://arxiv.org/pdf/2106.11257">butterfly all-reduce</a>. Once this
  process is successfully completed, each miner has a unified global averaged
  gradient that it can use to update its model weights.
</p>
