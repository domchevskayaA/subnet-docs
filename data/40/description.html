<h2>Chunking: Advancing Retrieval-Augmented Generation (RAG)</h2>

<h3>Background</h3>
<p>
  At VectorChat, our mission is to create the most immersive conversational AI
  experience. Our upcoming platform, Toffee, leverages Retrieval-Augmented
  Generation (RAG) to offer users seemingly endless memory, conversation length,
  and domain-specific knowledge.
</p>

<p>
  During the development of Toffee, we found that while many improvements had
  been made to RAG, chunking had been severely neglected. Existing solutions
  were either too rudimentary or resource-intensive, making the RAG pipeline
  both cost-prohibitive and less accurate. Traditional chunking methods (e.g.,
  chunking every X tokens with Y overlap) were cheaper but resulted in higher
  runtime costs. Unnecessary information was included as context for every LLM
  query, which is unaffordable when the user base of entertainment apps largely
  consists of free users and low-cost subscriptions. Conversely, existing
  semantic chunking solutions, such as Unstructured.io, were prohibitively
  expensive and slow, which would have severely limited the number of files
  users could upload.
</p>

<p>
  In response, to realize the vision of Toffee, our team had to design an
  algorithm that significantly outperformed the current offerings by industry
  leaders. We achieved this not by training proprietary models, but by
  leveraging the severely underdeveloped state of the field. The necessary
  information to develop solutions that match or exceed our current model is
  provided in this documentation.
</p>

<p>
  Our goal is to continue driving down costs, increasing accuracy, and enabling
  new possibilities. As LLMs begin to use larger and more diverse datasets
  (e.g., audio, image, video), the importance of intelligent chunking will only
  grow.
</p>

<p>
  Thus, we designed this subnet to have a straightforward, transparent, and fair
  incentive mechanism to surpass our own achievements. Explore the subnet
  architecture below to learn how responses are evaluated fairly.
</p>

<p>
  We believe the best solutions are yet to come, and we are excited to see how
  miners can push the boundaries of this technology!
</p>

<h3>Roadmap</h3>
<p>
  The following phases are not necessarily sequential and may occur
  concurrently.
</p>

<h4>Setup</h4>
<p>
  The initial phase of the subnet, containing the basic functionality of the
  subnet, verifying the incentive mechanism works as intended, and creating
  resources for miners, validators, and potential consumers to monitor the
  subnet.
</p>

<h5>Objectives:</h5>
<ul>
  <li>The first contest: the chunking of unstructured text</li>
  <li>Autoupdate for Miners and Validators</li>
  <li>Miner blacklist by Stake and Address</li>
  <li>Open-source Task API Framework to query the subnet</li>
  <li>
    Subnet Dashboard for Miners and Validators (see: subnet.chunking.com)
    <ul>
      <li>Display Subnet KPIs</li>
      <li>Periodic global benchmark</li>
      <li>Display subnet performance over time</li>
      <li>Chunking visualizer of all synthetic chunks produced</li>
      <li>Display all Miner and Validator data</li>
      <li>Show all tournament rounds: global, by validator, and by miner</li>
    </ul>
  </li>
  <li>
    Unique synthetic queries via Wikipedia + LLM, mitigating potential for
    lookup attacks and providing miners a way to detect and prevent relay mining
  </li>
  <li>Release extensive guides on how to build excellent chunkers</li>
  <li>
    Begin hosting multiple winner-take-all contests, for different resultant
    types of smart chunking
    <ul>
      <li>
        Unstructured images
        <ul>
          <li>image -> image</li>
          <li>image -> text</li>
        </ul>
      </li>
      <li>
        Unstructured audio
        <ul>
          <li>audio -> audio</li>
          <li>audio -> text</li>
        </ul>
      </li>
      <li>
        Unstructured video
        <ul>
          <li>video -> video</li>
          <li>video -> text</li>
        </ul>
      </li>
      <li>Special file types (PDFs, CSV, JSON -> text)</li>
      <li>
        Omnimodal (text, image, audio, video -> text, image, audio, video)
      </li>
    </ul>
  </li>
</ul>

<h4>Production</h4>
<p>
  The next phase of the subnet aims to make the intelligence incentivized by
  this subnet viable for commercial, enterprise, and personal use. Ensuring
  end-user data privacy becomes paramount.
</p>

<p>
  The incentive mechanism must change such that Validators and Miners never have
  access to the documents sent to be chunked, and such that no other parties
  ever gain the models created by Miners.
</p>

<p>But how might that work? See our (very tentative) approach.</p>

<h5>Objectives:</h5>
<ul>
  <li>
    Private Organic Query System
    <ul>
      <li>Finalize, collaborate, and share with other subnets in Bittensor</li>
      <li>Launch on testnet</li>
      <li>Release on mainnet</li>
    </ul>
  </li>
  <li>Launch of Chunking.com Task API, helping deliver organic demand</li>
  <li>Dashboard for Validators to monitor compensation and bandwidth use</li>
  <li>Achieve significant real world demand flowing into the subnet</li>
</ul>

<h4>ETL for RAG</h4>
<p>
  The third phase has the subnet expand to subsume the full Retrieval-Augmented
  Generation (RAG) pipeline, forming a complete connection between Bittensor and
  viable, real world demand.
</p>

<p>
  As all components of the subnet are meant to be used in RAG, the Evaluation
  will change to a standardized RAG benchmark, where the independent variable
  for any given contest is the subject of that very contest.
</p>

<h5>Objectives:</h5>
<ul>
  <li>
    Preprocessing Contests
    <ul>
      <li>Text and Structure Extraction</li>
    </ul>
  </li>
  <li>
    Embedding Contests
    <ul>
      <li>Text</li>
      <li>Audio</li>
      <li>Image</li>
      <li>Video</li>
    </ul>
  </li>
  <li>
    Vector DB Contests
    <ul>
      <li>Vector Search</li>
    </ul>
  </li>
</ul>
